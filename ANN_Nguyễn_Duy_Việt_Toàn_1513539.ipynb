{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Nguyễn Duy Việt Toàn_1513539.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naot97/dlht/blob/master/ANN_Nguy%E1%BB%85n_Duy_Vi%E1%BB%87t_To%C3%A0n_1513539.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kdU7yokAnD6_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ANs1Ac3GFOZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class parent layer\n",
        "class Layer:\n",
        "    X = np.array([])\n",
        "    y = np.array([])\n",
        "    denta_x = 0  \n",
        "    num_node = 0\n",
        "\n",
        "    def init_para():\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJF-LXq30CiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HERE IS SOME LINEAR AND NON-LINEAR(ACTIVATION) LAYERS :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "THUEv4oVF90K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#class Linear layer\n",
        "class Linear(Layer):\n",
        "    w = []\n",
        "    b = []  \n",
        "    def __init__(self, num_node):\n",
        "        super(Linear, self).__init__()\n",
        "        self.num_node = num_node \n",
        "\n",
        "    def init_para(self, num_node_input):\n",
        "        np.random.seed(1)\n",
        "        self.w = np.random.rand(num_node_input, self.num_node)\n",
        "        self.b = np.random.rand(1, self.num_node)\n",
        "\n",
        "    def forward(self):\n",
        "        if len(self.X.shape) != 2 :\n",
        "            self.X = self.X.reshape(self.X.shape[0],-1)\n",
        "        self.y = self.X.dot(self.w) + self.b\n",
        "        return self.y\n",
        "\n",
        "    def backward(self,denta_y):\n",
        "        denta_x = denta_y.dot(self.w.T)\n",
        "        denta_w = self.X.T.dot(denta_y)\n",
        "        denta_b = denta_y.mean(axis = 0)\n",
        "        return denta_x, denta_w, denta_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "140CLR5RHCsB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class Relu\n",
        "class Relu(Layer):\n",
        "    def forward(self):\n",
        "        self.y = np.maximum(self.X , 0)\n",
        "        return self.y\n",
        "\n",
        "    def init_para(self,num_node_input):\n",
        "        self.num_node = num_node_input\n",
        "\n",
        "    def backward(self, denta_y):\n",
        "        m = np.ones(self.X.shape)\n",
        "        m[self.X < 0] = 0\n",
        "        self.denta_x = denta_y * m\n",
        "        return self.denta_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgMUa4xOy0PQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class Softmax\n",
        "class Softmax(Layer):\n",
        "    def forward(self):\n",
        "        e_Z = np.exp(self.X - np.max(self.X, axis = -1, keepdims = True))\n",
        "        self.y = e_Z / e_Z.sum(axis = -1)\n",
        "        return self.y\n",
        "    def backward(self, denta_y):\n",
        "        self.denta_x = denta_y * self.y*(1 - self.y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09Wy3XCxTjKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class MaxPooling\n",
        "class MaxPooling(Layer):\n",
        "  def __init__(self,X_dim,size,stride):\n",
        "\n",
        "        self.d_X, self.h_X, self.w_X = X_dim\n",
        "        \n",
        "        self.params = []\n",
        "\n",
        "        self.size = size\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.h_out = (self.h_X - size)/stride + 1\n",
        "        self.w_out = (self.w_X - size)/stride + 1\n",
        "        \n",
        "\n",
        "        if not self.h_out.is_integer() or not self.w_out.is_integer():\n",
        "            raise Exception(\"Invalid dimensions!\")\n",
        "        \n",
        "        self.h_out,self.w_out  = int(self.h_out), int(self.w_out)\n",
        "        self.out_dim = (self.d_X,self.h_out,self.w_out)\n",
        "    \n",
        "  def forward(self,X):\n",
        "        self.n_X = X.shape[0]\n",
        "        X_reshaped = X.reshape(X.shape[0]*X.shape[1],1,X.shape[2],X.shape[3])\n",
        "\n",
        "        self.X_col = im2col_indices(X_reshaped, self.size, self.size, padding = 0, stride = self.stride)\n",
        "        \n",
        "        self.max_indexes = np.argmax(self.X_col,axis=0)\n",
        "        out = self.X_col[self.max_indexes,range(self.max_indexes.size)]\n",
        "\n",
        "        out = out.reshape(self.h_out,self.w_out,self.n_X,self.d_X).transpose(2,3,0,1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4SCVD4uEzKxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class 2d Convolution layer\n",
        "class Convolution(Layer):\n",
        "    filters = None\n",
        "    padding = False\n",
        "    filter_shape = (0,0)\n",
        "    num_filter = 0\n",
        "    X_conv = None\n",
        "    w_conv = None\n",
        "    def __init__(self, num_filter, filter_shape):\n",
        "        super(Convolution, self).__init__()\n",
        "        self.filter_shape = filter_shape\n",
        "        self.num_filter = num_filter\n",
        "    def init_para(self, input_shape):\n",
        "        self.filters = np.random.rand(self.num_filter, self.filter_shape[0], self.filter_shape[1], input_shape[-1])\n",
        "    def create_x_conv(self):\n",
        "        filter_shape = self.filter_shape\n",
        "        num_X , rows, cols, channels = self.X.shape\n",
        "        horz_blocks = cols - filter_shape[1] + 1\n",
        "        vert_blocks = rows - filter_shape[0] + 1\n",
        "        \n",
        "        output_vectors = np.zeros((num_X, horz_blocks * vert_blocks, filter_shape[0] * filter_shape[1] * channels))\n",
        "        for i_x in range(num_X) :\n",
        "          itr = 0\n",
        "          for v_b in range(vert_blocks):\n",
        "            for h_b in range(horz_blocks):\n",
        "                output_vectors[i_x ,itr, :] = self.X[i_x, v_b: v_b + filter_shape[0], h_b: h_b + filter_shape[1], : ].ravel()\n",
        "                itr += 1\n",
        "        self.X_conv = output_vectors\n",
        "        print(self.X_conv)\n",
        "\n",
        "    def create_w_conv(self):\n",
        "        row = X_shape[0] - filter.shape[0] + 1\n",
        "        col = X_shape[1] - filter.shape[1] + 1\n",
        "        output = np.zeros((row * col, X_shape[0]*X_shape[1] ))\n",
        "        for i in range(row):\n",
        "            for j in range(col):\n",
        "                temp = np.zeros(X_shape)\n",
        "                temp[i : i + filter.shape[0], j : j + filter.shape[1]] = filter\n",
        "                output[i*col + j,:] = temp.flatten()\n",
        "        self.w_conv = output\n",
        "\n",
        "    def forward(self):\n",
        "        #check X shape\n",
        "        if len(self.X.shape) == 3:\n",
        "            self.X = self.X.reshape((self.X.shape,1))\n",
        "            \n",
        "        # create x_conv and w_flatten\n",
        "        w_flatten = self.filters.reshape(self.num_filter,-1).T\n",
        "        self.create_x_conv()\n",
        "        print(self.X_conv.shape)\n",
        "        print(w_flatten.shape)\n",
        "        \n",
        "        #dot matrix\n",
        "        return np.dot(self.X_conv,w_flatten).reshape(self.X.shape[0] ,self.X.shape[1] - self.filter_shape[0] + 1 ,\n",
        "                                                     self.X.shape[2] - self.filter_shape[1] + 1, self.num_filter)\n",
        "        #dot matrix\n",
        "        #temp = self.X.shape[0] ,self.X.shape[1] - self.filter_shape[0] + 1 , self.X.shape[2] - self.filter_shape[1] + 1\n",
        "        #print(temp)\n",
        "        #output = np.zeros(temp + (self.num_filter,))\n",
        "        #for w_i in range(len(w_flatten)) :\n",
        "        #    output[:,:,:, w_i] = np.dot(self.X_conv,w_flatten[w_i]).reshape(temp)\n",
        "        #return output\n",
        "    \n",
        "        \n",
        "c = Convolution(2,(2,2))\n",
        "#c.init_para((100,4,4,10))\n",
        "#c.X = np.random.rand(100,4,4,10)\n",
        "c.filters = np.array([[[ [1],[3]],[[1],[2]]], [[[1],[4]],[[3],[2]]]])\n",
        "print(c.filters.shape)\n",
        "c.X = np.array( [[ [[1],[2],[3]],[[4],[5],[6]], [[1],[2],[3]] ]]  )\n",
        "print(c.X.shape)\n",
        "print(c.forward())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tU86zvIhYFDB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "a = np.array( [[[[1],[2],[3]],[[4],[5],[6]],[[1],[2],[3]]]] )\n",
        "k = np.array([ [ [[ 1,1 ] ] , [ [ 3,4 ]]] , [ [[ 1,3 ] ] , [ [ 2,2 ]]] ])\n",
        "print(k.shape)\n",
        "print(a.shape)\n",
        "tensor_a = tf.constant(a, tf.float32)\n",
        "tensor_k = tf.constant(k, tf.float32)\n",
        "\n",
        "tensor_res = tf.nn.convolution(tensor_a, tensor_k, padding='VALID')\n",
        "\n",
        "sess = tf.Session()\n",
        "res = sess.run(tensor_res)\n",
        "print(res)\n",
        "print(res.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMJa2WHr09tZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Class network and main program :"
      ]
    },
    {
      "metadata": {
        "id": "_kJR1cOMHMjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class ANN\n",
        "class ANN:\n",
        "    list_layer = []\n",
        "\n",
        "    def config_layers(self, X):\n",
        "        self.list_layer[0].init_para(len(X[0]))\n",
        "        for i in range(1,len(self.list_layer)) :\n",
        "            self.list_layer[i].init_para(self.list_layer[i - 1].num_node)\n",
        "    def add(self, layer):\n",
        "        self.list_layer.append(layer)\n",
        "\n",
        "    def loss(self, y, t):\n",
        "        N = len(y)\n",
        "        return ((t - y)**2).mean(axis = 0)/2\n",
        "\n",
        "    def grad_loss(self,y, t):\n",
        "        N = len(y)\n",
        "        return (y - t)/N\n",
        "\n",
        "    def gd(self,y ,t , eta):\n",
        "        denta_y = self.grad_loss(y, t)\n",
        "        n = len(self.list_layer)\n",
        "        for i in range(n-1,-1,-1):\n",
        "            if type(self.list_layer[i]) is Linear :\n",
        "                denta_y,denta_w,denta_b = self.list_layer[i].backward(denta_y)\n",
        "                self.list_layer[i].w -= eta*denta_w\n",
        "                self.list_layer[i].b -= eta*denta_b\n",
        "            else :\n",
        "                denta_y = self.list_layer[i].backward(denta_y)\n",
        "\n",
        "    def forward(self,X):\n",
        "        inp = X\n",
        "        for layer in self.list_layer:\n",
        "            layer.X = inp\n",
        "            inp = layer.forward()\n",
        "        return inp \n",
        "    def fit(self, numEpoches, X, t, learning_rate, verbose = False):\n",
        "        # set number node for layers \n",
        "        self.config_layers(X);\n",
        "        \n",
        "        best_loss = None\n",
        "        best_model = None\n",
        "        #train\n",
        "        for i in range(numEpoches):\n",
        "            X_train, X_test, t_train , t_test = train_test_split(X, t, test_size=0.33)\n",
        "            #forward\n",
        "            y_test = self.forward(X_test)\n",
        "            y_train  = self.forward(X_train)\n",
        "            # print loss and save best model\n",
        "            loss = self.loss(y_test, t_test)\n",
        "            if  (best_loss is None) or (loss.sum(axis = 0) < best_loss.sum(axis = 0)):\n",
        "                best_loss = loss\n",
        "                best_model = copy.deepcopy(self)\n",
        "            if i % 1000 == 0 and verbose:\n",
        "                print('Epoch [%d/%d] Loss: '%(i+1, numEpoches), loss)\n",
        "            # gradient decent\n",
        "            self.gd(y_train, t_train, learning_rate)\n",
        "        return best_model,best_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tpm2YsRkHQAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#read data\n",
        "def readData():\n",
        "    #boston = datasets.load_boston()\n",
        "    #X = boston.data\n",
        "    #label = boston.target.reshape((506,1))\n",
        "\n",
        "    X = np.random.rand(1000, 2) \n",
        "    label1 = 4 + X.dot(np.array([[5,2],[1,3]])) \n",
        "    label = np.array([])\n",
        "    for l in label1:\n",
        "        label = np.append(label,np.fromiter(map(lambda x : x if x >= 0 else 0,l), dtype = np.float), axis=0 )\n",
        "    label = label.reshape(1000,2)\n",
        "    return X,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FdXgc3iMjzqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#main program\n",
        "def main(X, label, eta, iter):\n",
        "    # create model\n",
        "    model = ANN()\n",
        "    model.add(Linear(num_node = 2))\n",
        "    model.add(Relu())\n",
        "    \n",
        "    best,loss = model.fit(iter, X, label, 1)\n",
        "    print(loss)\n",
        "    print(best.list_layer[0].w)\n",
        "    print(best.list_layer[0].b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raiWo8ChpT2t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eta = 0.001\n",
        "iter = 150001\n",
        "X,label = readData()\n",
        "main(X, label, eta,iter)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}